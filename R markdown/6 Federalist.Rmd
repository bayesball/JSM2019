---
title: "Federalist Paper Study"
author: "Jim Albert"
date: "July 2019"
output:
  beamer_presentation:
    includes:
      in_header: header_pagenrs.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, warnings = FALSE)
```

## Famous Bayesian Study

-- Mosteller and Wallace (1963)

-- Authorship problem:  85 Federalist papers wrote to promote ratification of U.S. constitution

-- Some were written by Alexander Hamilton and some were written by James Madison

-- Who wrote the "unknown" Federalist papers -- Madison or  Hamilton?

-- Illustrated Bayesian reasoning to determine authorship

## Focus on the "filler words"

-- Use of some words depend on the content of the essay

-- Other words, so-called filler words, are less influenced by the essay content

-- Focus on the use of the word "can" by Hamilton

## Read data

```{r}
library(tidyverse)
d <- read_csv("Hamilton_can.csv")
head(d)
```

## Graph of the rates of "can" in 49 essays

```{r, echo = FALSE}
ggplot(d, aes(Rate)) + 
  geom_histogram(bins = 10,
                 color = "white",
                 fill = "red")
```

## Poisson model

-- Assume the number of occurrences of "can" in the $j$th document $y_j$ is Poisson with mean $n_j \lambda / 1000$.

-- $\lambda$ is true rate of "can" among 1000 words

-- Poisson sampling density

$$
f(y_j | \lambda) = \frac{(n_j \lambda / 1000)^{y_j} \exp(-n_j \lambda / 1000)}{y_j!}.
$$

## Prior

-- Assume know little about location of $\lambda$

-- Assign $\lambda$ a Gamma prior with shape 0.001 and scale 0.001


## Write JAGS script defining the model

```{r}
modelString = "
model{
for (i in 1:N) {
y[i] ~ dpois(n[i] * lambda / 1000)
}
lambda ~ dgamma(0.001, 0.001)
}
"
```

## Read in the data

```{r}
y <- d$N
n <- d$Total
the_data <- list("y" = y, "n" = n, N = length(y))
```

## Use runjags package

Simulate 5000 draws from posterior of $\lambda$

```{r}
library(runjags)
jags <- run.jags(modelString,
                 data = the_data,
                 monitor = c("lambda"),
                 n.chains = 1,
                 burnin = 2000,
                 sample = 5000,
                 silent.jags = TRUE)
```


## Checking for Overdispersion

- Simulate replicated data $y^*$ from posterior predictive
- Compute standard deviation of $y^*$
- Repeat for 5000 iterations

```{r}
library(coda)
post <- as.mcmc(jags)
one_rep <- function(j){
  lambda <- post[j]
  sd(rpois(length(y), n * lambda / 1000))
}
sapply(1:5000, one_rep) -> SD
```

## Checking for Overdispersion

```{r, echo = FALSE}
library(CalledStrike)
ggplot(data.frame(sd = SD), aes(sd)) +
  geom_histogram(color = "black", fill = "white",
                 bins = 15) + increasefont() +
  geom_vline(xintercept = sd(y), size = 3 ) +
  annotate('text', x = 4, y = 950, 
           label = "Observed", size = 7) +
  ggtitle(paste("Tail Probability =",
                mean(SD >= sd(y))))
```

## Consider Negative Binomial sampling

- Assume $y_j$ is Negative Binomial with parameters $p_j$ and $\alpha$

- Reparametrize $p_j$ to $\beta$

$$
p_j = \frac{\beta}{\beta + n_j / 1000}.
$$

$$
f(y_j | \alpha, \beta) = \frac{\Gamma(y_j + \alpha)}{\Gamma(\alpha)} p_j^\alpha (1 - p_j)^{y_j}
$$

## NB is Generalization of Poisson

- Mean of $y_j$ is
$$
E(y_j) = \mu_j =  \frac{n_j}{1000}\frac{\alpha}{\beta}
$$

- Variance of $y_j$ is

$$
Var(y_j) = \mu_j \left(1 + \frac{n_j}{1000 \beta}\right).
$$

- Parameter $\mu = \alpha / \beta$ is true rate per 1000 words

- $\beta$ is overdispersion parameter



## Model script for Negative Binomial sampling

```{r}
modelString = "
model{
for(i in 1:N){
p[i] <- beta / (beta + n[i] / 1000)
y[i] ~ dnegbin(p[i], alpha)
}	
mu <- alpha / beta
alpha ~ dgamma(.001, .001)
beta ~ dgamma(.001, .001)
}
"
```


## Run JAGS

```{r}
the_data <- list("y" = y, "n" = n, N = length(y))
jags <- run.jags(modelString,
                 data = the_data,
                 monitor = c("alpha", "beta", "mu"),
                 n.chains = 1,
                 burnin = 2000,
                 sample = 5000,
                 silent.jags = TRUE)
```

## PP check using SD as checking function
                 
```{r, echo = FALSE}
post <- as.data.frame(as.mcmc(jags))
one_rep <- function(j){
  p <- post$beta[j] / (post$beta[j] + n / 1000)
  sd(rnbinom(length(y), size = post$alpha[j], prob = p))
}
sapply(1:5000, one_rep) -> SD
ggplot(data.frame(sd = SD), aes(sd)) +
  geom_histogram(color = "black", fill = "white",
                 bins = 15) + increasefont() +
  geom_vline(xintercept = sd(y), size = 3 ) +
  annotate('text', x = 7, y = 1250, 
           label = "Observed", size = 7) +
  ggtitle(paste("Tail Probability =",
                mean(SD >= sd(y))))
```
                 
## Posterior Summary of Mean Parameter $\mu = \alpha / \beta$

```{r, echo = FALSE}
library(coda)
plot(jags, vars = "mu")
```

## Compare Authors

- Suppose want to compare Hamilton and Madison's use of the word "can"

- Collect counts \{$y_{1j}$\} for Hamilton, counts \{$y_{2j}$\} for Madison

- Assume samples from independent NB($\alpha_1, \beta_1$),
NB($\alpha_2, \beta_2$) distributions

- Interested in comparing mean rates of "can"
$$
R_{Madison} = \frac{\alpha_2/\beta_2}{\alpha_1/\beta_1}
$$

## Model script -- two author comparison

```{r}
modelString = "
model{
for(i in 1:N1){
p1[i] <- beta1 / (beta1 + n1[i] / 1000)
y1[i] ~ dnegbin(p1[i], alpha1)
}	
for(i in 1:N2){
p2[i] <- beta2 / (beta2 + n2[i] / 1000)
y2[i] ~ dnegbin(p2[i], alpha2)
}	
alpha1 ~ dgamma(.001, .001)
beta1 ~ dgamma(.001, .001) 
alpha2 ~ dgamma(.001, .001)
beta2 ~ dgamma(.001, .001)
ratio <- (alpha2 / beta2) / (alpha1 / beta1)
}
"
```

## Read in data

```{r}
d1 <- read_csv("Hamilton_can.csv")
d2 <- read_csv("Madison_can.csv")

the_data <- list("y1" = d1$N, "n1" = d1$Total, 
                 "N1" = length(d1$N),
                 "y2" = d2$N, "n2" = d2$Total,
                 "N2" = length(d2$N))
```

## JAGS run - compare use of "can"

```{r, echo = FALSE}
jags <- run.jags(modelString,
                 data = the_data,
                 monitor = c("ratio"),
                 n.chains = 1,
                 burnin = 2000,
                 sample = 5000,
                 silent.jags = TRUE)
plot(jags)
```

## Compare Hamilton and Madison across many words

```{r, echo = FALSE}
d <- read_csv("fed_word_study.csv")
bayes_one_word <- function(theword){
  d1 <- filter(d, Authorship == "Hamilton",
               word == theword)
  d2 <- filter(d, Authorship == "Madison",
               word == theword)
  the_data <- list("y1" = d1$N, "n1" = d1$Total, 
                   "N1" = length(d1$N),
                   "y2" = d2$N, "n2" = d2$Total,
                   "N2" = length(d2$N))
  jags <- run.jags(modelString,
                   data = the_data,
                   monitor = c("ratio"),
                   n.chains = 1,
                   burnin = 2000,
                   sample = 5000,
                 silent.jags = TRUE)
  quantile(as.mcmc(jags$mcmc), c(.025, .5, .975))
}
```

```{r}
word_list <- c("by", "from", "to", "an", "any", "may",
               "his", "upon", "also", "can",
               "of", "on", "there", "this")
word_list <- sort(word_list)
S <- sapply(word_list, bayes_one_word)
df <- data.frame(Word = word_list,
                 LO = S[1, ],
                 M = S[2, ],
                 HI = S[3, ])
```

## Graph

```{r, echo = FALSE}
ggplot(df, aes(x = Word, y = M)) +
  geom_errorbar(aes(ymin = LO, ymax = HI), width = 0.3, size = 1) +
  geom_point() + coord_flip() +
  increasefont() +
  ylab("Ratio favoring Madison") +
  geom_hline(yintercept = 1)
```

## Takeaways

- Hamilton more likely to use words "upon", "to", "this",
"there", "any", and "an" 

- Madison more likely to use "on",
"by", and "also" 

- Inconclusive for the remaining words (may, his, from, can, and also) 

